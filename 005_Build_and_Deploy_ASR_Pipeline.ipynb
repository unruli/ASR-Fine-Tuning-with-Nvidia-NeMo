{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 5.0 Build and Deploy an ASR Pipeline with Riva\n",
    "In this notebook, you'll build and deploy an ASR pipeline on NVIDIA Riva with freely available components, which have been pretrained with NVIDIA NeMo.  These components include an acoustic model, an n-gram language model, a punctuation and capitalization model, and an inverse text normalization model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=images/riva/ASR_pipeline.PNG width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[5.1 Riva ServiceMaker](#5.1-Riva-ServiceMaker)<br>**\n",
    "**[5.2 Download the ASR Pipeline Models from NGC](#5.2-Download-the-ASR-Pipeline-Models-from-NGC)<br>**\n",
    "**[5.3 `riva-build`](#5.3-riva-build)<br>**\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[5.3.1 Identify the File Paths Required](#5.3.1-Identify-the-File-Paths-Required)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[5.3.2 `docker run` Syntax](#5.3.2-docker-run-Syntax)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[5.3.3 `riva-build speech_recognition` Syntax](#5.3.3-riva-build-speech_recognition-Syntax)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[5.3.4 Exercise: Build a Punctuation and Capitalization RMIR](#5.3.4-Exercise:-Build-a-Punctuation-and-Capitalization-RMIR)<br>\n",
    "**[5.4 `riva-deploy`](#5.4-riva-deploy)<br>**\n",
    "**[5.5 Start the Riva Server ](#5.5-Start-the-Riva-Server)**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[5.5.1 Exercise: Configure the `config.sh` File](#5.5.1-Exercise:-Configure-the-config.sh-File)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[5.5.2 Start the Server](#5.5.2-Start-the-Server)<br>\n",
    "**[5.6 Run Inference](#5.6-Run-Inference)**<br>\n",
    "**[5.7 Stop the Riva Server](#5.7-Stop-the-Riva-Server)**<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Notebook Dependencies\n",
    "The steps in this notebook assume that you have:\n",
    "\n",
    "1. **NGC Credentials Installed**<br>Be sure you have added your NGC credential using the [NGC Setup notebook](003_NGC_Setup.ipynb)\n",
    "1. **Riva Quick Start resources folder has been downloaded**<br>Execute the following cell to make sure you have this folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Riva Riva Skills Quick Start resource folder already downloaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set the path to the Riva Skills Quick Start resource folder\n",
    "RIVA_DIR = \"riva_quickstart_v2.11.0\"\n",
    "\n",
    "# Downloads the Riva Skills Quick Start resource folder (overwrite if necessary)\n",
    "if os.path.exists(RIVA_DIR):\n",
    "    print(\"Riva Riva Skills Quick Start resource folder already downloaded\")\n",
    "else:\n",
    "    print(\"Downloading the Riva Skills Quick Start resource folder\")\n",
    "    !ngc registry resource download-version \"nvidia/riva/riva_quickstart:2.11.0\"\n",
    "    # Make special modification required for our docker-in-docker course environment\n",
    "    !sed -i '/--name riva-service-maker*/i \\              --network host \\\\' $RIVA_DIR/riva_init.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# 5.1 Riva ServiceMaker\n",
    "\n",
    "Riva ServiceMaker is a Docker container image that includes a set of tools that aggregates all the necessary artifacts (models, files, configurations, and user settings) for Riva deployment to a target environment. It has two main components: `riva-build` and `riva-deploy`.\n",
    "\n",
    "The `riva_init.sh` script we used in the Quick Start notebook used the ServiceMaker `riva-deploy` tool behind the scenes to deploy prebuilt `.rmir` models for us. In this notebook, we'll use the Riva ServiceMaker container directly to build models and deploy to a target environment using the `riva-build` and `riva-deploy` calls.  \n",
    "\n",
    "With these tools, we have the option of customizing our ASR pipeline with a variety of models as needed.  We trade the abstraction of simply calling `riva_init.sh` for flexibility going forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=images/riva/servicemaker.png width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--\n",
    "# 5.2 Download the ASR Pipeline Models from NGC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models for the ASR pipeline can be downloaded from NGC.  To see a list of models available, search the NGC catalog for the [Riva Speech Skills collection](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/riva/collections/riva-speech/entities), or search for individual models. \n",
    "\n",
    "The next several cells use the NGC command line utility to download various models with the API key you've loaded. First, though, let's define the directories into which we want to download our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Overarching model diretory\n",
    "MODEL_LOC = \"/dli/task/asr-models\"\n",
    "# Directory for the components of the prebuilt, OOTB models\n",
    "DEFAULT_MODEL_LOC = os.path.join(MODEL_LOC, \"default-models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download Conformer-CTC\n",
    "Download the deployable `Conformer-CTC-L-en-US-ASR-set-4p0.riva` ASR model, which we'll deploy for our out-of-the-box example with Riva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading the deployable Acoustic Model\n",
      "{\n",
      "    \"download_end\": \"2025-03-30 05:49:30\",\n",
      "    \"download_start\": \"2025-03-30 05:49:29\",\n",
      "    \"download_time\": \"0s\",\n",
      "    \"files_downloaded\": 1,\n",
      "    \"local_path\": \"/dli/task/asr-models/default-models/speechtotext_en_us_conformer_vdeployable_v4.0\",\n",
      "    \"size_downloaded\": \"110 B\",\n",
      "    \"status\": \"COMPLETED\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "AM_DIR = \"speechtotext_en_us_conformer_vdeployable_v4.0\"\n",
    "AM_PATH = os.path.join(DEFAULT_MODEL_LOC, AM_DIR)\n",
    "\n",
    "if os.path.exists(AM_PATH):\n",
    "    print(\"Deployable Acoustic Model exists, skipping download\")\n",
    "else:\n",
    "    print(\"Downloading the deployable Acoustic Model\")\n",
    "    !ngc registry model \\\n",
    "       download-version \"nvidia/riva/speechtotext_en_us_conformer:deployable_v4.0\" \\\n",
    "       --dest $DEFAULT_MODEL_LOC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download Inverse Text Normalization Files\n",
    "Inverse text normalization (ITN) converts spoken-domain automatic speech recognition (ASR) output into written-domain text to improve the readability of the ASR output. See [this paper](https://arxiv.org/pdf/2104.05055.pdf) for detailed information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading the ITN Model\n",
      "{\n",
      "    \"download_end\": \"2025-03-30 05:49:52\",\n",
      "    \"download_start\": \"2025-03-30 05:49:51\",\n",
      "    \"download_time\": \"0s\",\n",
      "    \"files_downloaded\": 2,\n",
      "    \"local_path\": \"/dli/task/asr-models/default-models/inverse_normalization_en_us_vdeployable_v2.0\",\n",
      "    \"size_downloaded\": \"220 B\",\n",
      "    \"status\": \"COMPLETED\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "ITN_DIR = \"inverse_normalization_en_us_vdeployable_v2.0\"\n",
    "ITN_PATH = os.path.join(DEFAULT_MODEL_LOC, ITN_DIR)\n",
    "\n",
    "if os.path.exists(ITN_PATH):\n",
    "    print(\"ITN Model exists, skipping download\")\n",
    "else:\n",
    "    print(\"Downloading the ITN Model\")\n",
    "    !ngc registry model \\\n",
    "       download-version \"nvidia/riva/inverse_normalization_en_us:deployable_v2.0\" \\\n",
    "       --dest $DEFAULT_MODEL_LOC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download Punctuation and Capitalization Model\n",
    "Adding this punctuation and capitalization model will improve the readability of the transcripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading the Punctuation and Capitalization Model\n",
      "{\n",
      "    \"download_end\": \"2025-03-30 05:50:16\",\n",
      "    \"download_start\": \"2025-03-30 05:50:16\",\n",
      "    \"download_time\": \"0s\",\n",
      "    \"files_downloaded\": 1,\n",
      "    \"local_path\": \"/dli/task/asr-models/default-models/punctuationcapitalization_en_us_bert_base_vdeployable_v3.0\",\n",
      "    \"size_downloaded\": \"110 B\",\n",
      "    \"status\": \"COMPLETED\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "PC_DIR = \"punctuationcapitalization_en_us_bert_base_vdeployable_v3.0\"\n",
    "PC_PATH = os.path.join(DEFAULT_MODEL_LOC, PC_DIR)\n",
    "\n",
    "if os.path.exists(PC_PATH):\n",
    "    print(\"Punctuation and Capitalization Model exists, skipping download\")\n",
    "else:\n",
    "    print(\"Downloading the Punctuation and Capitalization Model\")\n",
    "    !ngc registry model \\\n",
    "        download-version \"nvidia/riva/punctuationcapitalization_en_us_bert_base:deployable_v3.0\" \\\n",
    "        --dest $DEFAULT_MODEL_LOC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Download Language Model Files\n",
    "The language model files we need are preloaded in this course to save time, so you don't need to pull them. For reference, we downloaded them as follows: \n",
    "```bash\n",
    "LM_DIR = \"speechtotext_en_us_lm_vdeployable_v4.1\"\n",
    "LM_PATH = os.path.join(DEFAULT_MODEL_LOC, LM_DIR)\n",
    "\n",
    "if os.path.exists(LM_PATH):\n",
    "    print(\"Language Model exists, skipping download\")\n",
    "else:\n",
    "    print(\"Downloading the Language Model\")\n",
    "    !ngc registry model download-version \"nvidia/riva/speechtotext_en_us_lm:deployable_v4.1\" --dest $DEFAULT_MODEL_LOC\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 20\n",
      "drwx------ 2 root 4096 Mar 30 05:49 inverse_normalization_en_us_vdeployable_v2.0\n",
      "drwxr-xr-x 9 1102 4096 Mar 30 05:35 models\n",
      "drwx------ 2 root 4096 Mar 30 05:50 punctuationcapitalization_en_us_bert_base_vdeployable_v3.0\n",
      "drwx------ 2 root 4096 Mar 30 05:49 speechtotext_en_us_conformer_vdeployable_v4.0\n",
      "drwx------ 2 root 4096 Mar 30 05:37 speechtotext_en_us_lm_vdeployable_v4.1\n"
     ]
    }
   ],
   "source": [
    "# Check the downloads.\n",
    "!ls -g $DEFAULT_MODEL_LOC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list should include the following model directories needed for our pipeline:\n",
    "\n",
    "- **speechtotext_en_us_conformer_vdeployable_v4.0** (accoustic model)\n",
    "- **speechtotext_en_us_lm_vdeployable_v4.1** (language model)\n",
    "- **punctuationcapitalization_en_us_bert_base_vdeployable_v3.0** (punctuation and capitalization model)\n",
    "- **inverse_normalization_en_us_vdeployable_v2.0** (inverse text normalization model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# 5.3 `riva-build`\n",
    "\n",
    "The `riva-build` step helps build a Riva-ready version of the model. Its only output is an intermediate format (called an RMIR) of an end-to-end pipeline for the supported services within Riva. RMIR stands for  for **R**iva **M**odel **I**ntermediate **R**epresentation.\n",
    "\n",
    "`riva-build` is responsible for combining one or more exported models (`.riva` files) into a single file containing an intermediate RMIR format  (`.rmir`). This file contains a deployment-agnostic specification of the whole end-to-end pipeline along with all the assets required for the final deployment and inference. \n",
    "\n",
    "For more information on `riva-build`, refer to the [Riva Build documentation](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/model-overview.html#riva-build).  For ASR, we'll use the `riva-build speech_recognition` task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3.1 Identify the File Paths Required\n",
    "For our ASR n-gram language model project, we need to aggregate the following elements: \n",
    "   - An _acoustic_ model file in the `.riva` format\n",
    "   - A _language_ binary model file in the `.binary` format<br>\n",
    "   - A decoder vocabulary file \n",
    "   - Weighted Finite State Transducer (WFST) tokenizer and verbalizer files for Inverse Text Normalization (ITN). For more information on WFST and ITN, refer to the [NeMo Inverse Text Normalization: From Development to Production](https://arxiv.org/pdf/2104.05055.pdf) paper.  \n",
    "   - A punctuation and capitalization (P&C) model. This isn't strictly necessary, but it improves the readability of ASR transcripts. \n",
    "   \n",
    "Start by setting up the paths to the files previously downloaded for the project.  Then pull the ServiceMaker Docker container. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# ServiceMaker Docker container\n",
    "RIVA_SM_CONTAINER = \"nvcr.io/nvidia/riva/riva-speech:2.11.0-servicemaker\"\n",
    "\n",
    "# All model paths relative to Riva ServiceMaker Docker container include the _SM suffix\n",
    "\n",
    "# Model base directory w.r.t. both the host and the ServiceMaker container\n",
    "ASR_MODEL_DIR = os.path.abspath(\"asr-models/default-models\")\n",
    "ASR_MODEL_DIR_SM = \"/servicemaker-dev\" # Path where we mount the downloaded ASR models in the ServiceMaker container\n",
    "\n",
    "# Relative path to Acoustic Model\n",
    "AM_DIR = \"speechtotext_en_us_conformer_vdeployable_v4.0\"\n",
    "AM_SM  = os.path.join(ASR_MODEL_DIR_SM, AM_DIR, \"Conformer-CTC-L-en-US-ASR-set-4p0.riva\")\n",
    "\n",
    "# Relative path to LM model artifacts\n",
    "LM_DIR = \"speechtotext_en_us_lm_vdeployable_v4.1\"\n",
    "DECODING_LM_BINARY_SM = os.path.join(ASR_MODEL_DIR_SM, LM_DIR, \"riva_asr_train_datasets_3gram.binary\")\n",
    "DECODING_VOCAB_SM = os.path.join(ASR_MODEL_DIR_SM, LM_DIR, \"flashlight_decoder_vocab.txt\")\n",
    "\n",
    "# Relative path to WSFT artifacts for inverse text normalization\n",
    "ITN_DIR = \"inverse_normalization_en_us_vdeployable_v2.0\"\n",
    "WFST_TOKENIZER_MODEL_SM  = os.path.join(ASR_MODEL_DIR_SM, ITN_DIR, \"tokenize_and_classify.far\")\n",
    "WFST_VERBALIZER_MODEL_SM = os.path.join(ASR_MODEL_DIR_SM, ITN_DIR, \"verbalize.far\")\n",
    "SPEECH_HINTS_MODEL_SM = os.path.join(ASR_MODEL_DIR_SM, ITN_DIR, \"speech_class.far\")\n",
    "\n",
    "# Relative path to Punctuation and Capitalization Model\n",
    "PC_DIR = \"punctuationcapitalization_en_us_bert_base_vdeployable_v3.0\"\n",
    "PC_SM  = os.path.join(ASR_MODEL_DIR_SM, PC_DIR, \"bert-base_PnC_en-US_3.0.riva\")\n",
    "\n",
    "# Relative paths where the generated .rmir files will be stored\n",
    "!mkdir -p $ASR_MODEL_DIR/rmir\n",
    "ASR_RMIR_DIR_SM = os.path.join(ASR_MODEL_DIR_SM, \"rmir\")\n",
    "ASR_RMIR_SM = os.path.join(ASR_RMIR_DIR_SM, \"asr_lm_itn_offline.rmir\")\n",
    "PC_RMIR_SM = os.path.join(ASR_RMIR_DIR_SM, \"p_and_c.rmir\")\n",
    "\n",
    "# Key that model is encrypted with\n",
    "KEY = \"tlt_encode\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0-servicemaker: Pulling from nvidia/riva/riva-speech\n",
      "Digest: sha256:7831bcd8deb4e18f6af937730833c93ee10e706add3b9da0572f56c94d292074\n",
      "Status: Image is up to date for nvcr.io/nvidia/riva/riva-speech:2.11.0-servicemaker\n",
      "nvcr.io/nvidia/riva/riva-speech:2.11.0-servicemaker\n"
     ]
    }
   ],
   "source": [
    "# Get the ServiceMaker Docker container (should already have been pulled in the Quick Start example)\n",
    "! docker pull $RIVA_SM_CONTAINER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5.3.2 `docker run` Syntax\n",
    "\n",
    "We use a [docker run](https://docs.docker.com/engine/reference/commandline/run/) command to run the ServiceMaker container with the basic syntax:\n",
    "```text\n",
    "    docker run [OPTIONS] IMAGE [COMMAND] [ARG...]       \n",
    "```\n",
    "\n",
    "which becomes for us:\n",
    "\n",
    "```text\n",
    "    docker run --rm --gpus 1 \\\n",
    "        -v $ASR_MODEL_DIR:/servicemaker-dev \\\n",
    "        $RIVA_SM_CONTAINER -- \\\n",
    "        riva-build speech_recognition \\       \n",
    "```\n",
    "\n",
    "Here's a breakdown of the command and options we are using:<br>\n",
    "- **docker run** - command to run the container\n",
    "- **--rm** - tells docker to clean up after the container runs\n",
    "- **--gpus 1** - specifies number of GPUs (just one in this case)\n",
    "- **-v \\$MODEL_LOC:/servicemaker-dev** - shared volume; we are mapping our `ASR_MODEL_DIR` on the host to `/servicemaker-dev` inside the ServiceMaker container.\n",
    "- **\\$RIVA_SM_CONTAINER** - the container image we just pulled from NGC\n",
    "- **riva-build speech_recognition** - This command and all its arguments are run inside the ServiceMaker container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5.3.3 `riva-build speech_recognition` Syntax\n",
    "We can get some help on the `riva-build speech_recognition` syntax from the [Pipeline Configuration documentation](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/asr/asr-pipeline-configuration.html?highlight=pipeline%20configuration).  For our use case, the basic syntax is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "    riva-build speech_recognition \\\n",
    "        output-dir-for-rmir/model.rmir:key \\\n",
    "        dir-for-riva/acoustic_model.riva:key \\\n",
    "        --decoding_language_model_binary=lm_model.binary          \n",
    "```\n",
    "    \n",
    "which becomes:\n",
    "\n",
    "```text\n",
    "    riva-build speech_recognition \\\n",
    "        $ASR_RMIR_SM:$KEY \\\n",
    "        $AM_SM:$KEY \\\n",
    "        --decoding_language_model_binary=$DECODING_LM_BINARY_SM          \n",
    "```    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the location of the model in the `--decoding_language_model_binary=` argument is relative to the container location, not the host.  Since we've mapped `$ASR_MODEL_DIR` on the host to `/servicemaker-dev` in the container, and defined `DECODING_LM_BINARY_SM` with respect to the container, specifying `$DECODING_LM_BINARY_SM` will ultimately map to `/dli/task/asr-models/speechtotext_en_us_lm_vdeployable_v4.1/riva_asr_train_datasets_3gram.binary`.  \n",
    "\n",
    "There are a lot of arguments available to the `riva-build speech_recognition` command.  A comprehensive list can be found in the [Riva-build Optional Parameters documentation](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/asr/asr-pipeline-configuration.html?highlight=pipeline%20configuration#riva-build-optional-parameters). \n",
    "\n",
    "`riva-build` supports language models in the `.riva` and `.arpa` formats as well as `.binary`. If your language model is in the `.riva` format, define `DECODING_LM_RIVA_SM` analogously to `DECODING_LM_BINARY_SM` and replace `--decoding_language_model_binary=$DECODING_LM_BINARY_SM` with `$DECODING_LM_RIVA_SM:$KEY` . If your language model is in the `.arpa` format, define `DECODING_LM_ARPA_SM` analogously to `DECODING_LM_BINARY_SM` and replace `--decoding_language_model_binary=$DECODING_LM_BINARY_SM` with `--decoding_language_model_arpa=$DECODING_LM_ARPA_SM`.\n",
    "\n",
    "Refer to the [Riva ASR Pipeline Configuration documentation](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/asr/asr-pipeline-configuration.html) if you want to build an ASR pipeline for a supported language other than US English. To obtain the proper `riva-build` parameters for your particular application, select the acoustic model (the parameters below assume Conformer-CTC), language, and pipeline type (offline for the purposes of this tutorial) from the interactive web menu at the bottom of the first section of the page.\n",
    "\n",
    "Execute the following cell to build an `.rmir` file from a `.binary`-formatted n-gram language model file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2920761605.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[12], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    ngc registry model download-version nvidia/riva/riva_asr_conformer_ctc_en_us:deployable_v4.0 \\\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================\n",
      "=== Riva Speech Skills ===\n",
      "==========================\n",
      "\n",
      "NVIDIA Release  (build 59018721)\n",
      "Copyright (c) 2016-2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
      "\n",
      "Copyright (c) 2018-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
      "\n",
      "https://developer.nvidia.com/tensorrt\n",
      "\n",
      "Various files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n",
      "\n",
      "This container image and its contents are governed by the NVIDIA Deep Learning Container License.\n",
      "By pulling and using the container, you accept the terms and conditions of this license:\n",
      "https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\n",
      "\n",
      "To install Python sample dependencies, run /opt/tensorrt/python/python_setup.sh\n",
      "\n",
      "To install the open-source samples corresponding to this TensorRT release version\n",
      "run /opt/tensorrt/install_opensource.sh.  To build the open source parsers,\n",
      "plugins, and samples for current top-of-tree on master or a different branch,\n",
      "run /opt/tensorrt/install_opensource.sh -b <branch>\n",
      "See https://github.com/NVIDIA/TensorRT for more information.\n",
      "\n",
      "NOTE: CUDA Forward Compatibility mode ENABLED.\n",
      "  Using CUDA 11.8 driver version 520.61.05 with kernel driver version 510.47.03.\n",
      "  See https://docs.nvidia.com/deploy/cuda-compatibility/ for details.\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/tarfile.py\", line 1674, in gzopen\n",
      "    t = cls.taropen(name, mode, fileobj, **kwargs)\n",
      "  File \"/usr/lib/python3.8/tarfile.py\", line 1651, in taropen\n",
      "    return cls(name, mode, fileobj, **kwargs)\n",
      "  File \"/usr/lib/python3.8/tarfile.py\", line 1514, in __init__\n",
      "    self.firstmember = self.next()\n",
      "  File \"/usr/lib/python3.8/tarfile.py\", line 2318, in next\n",
      "    tarinfo = self.tarinfo.fromtarfile(self)\n",
      "  File \"/usr/lib/python3.8/tarfile.py\", line 1104, in fromtarfile\n",
      "    buf = tarfile.fileobj.read(BLOCKSIZE)\n",
      "  File \"/usr/lib/python3.8/gzip.py\", line 292, in read\n",
      "    return self._buffer.read(size)\n",
      "  File \"/usr/lib/python3.8/_compression.py\", line 68, in readinto\n",
      "    data = self.read(len(byte_view))\n",
      "  File \"/usr/lib/python3.8/gzip.py\", line 479, in read\n",
      "    if not self._read_gzip_header():\n",
      "  File \"/usr/lib/python3.8/gzip.py\", line 427, in _read_gzip_header\n",
      "    raise BadGzipFile('Not a gzipped file (%r)' % magic)\n",
      "gzip.BadGzipFile: Not a gzipped file (b'<?')\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen eff.core.archive>\", line 764, in extract\n",
      "  File \"/usr/lib/python3.8/tarfile.py\", line 1621, in open\n",
      "    return func(name, filemode, fileobj, **kwargs)\n",
      "  File \"/usr/lib/python3.8/tarfile.py\", line 1678, in gzopen\n",
      "    raise ReadError(\"not a gzip file\")\n",
      "tarfile.ReadError: not a gzip file\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/riva-build\", line 8, in <module>\n",
      "    sys.exit(build())\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/servicemaker/cli/build.py\", line 96, in build\n",
      "    nm = model(input_filename, pipeline_config=pipeline_config, encryption_key=input_key)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/servicemaker/readers/tlt.py\", line 76, in __init__\n",
      "    read_eff_manifest(self, self.source_file)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/servicemaker/readers/tlt.py\", line 65, in read_eff_manifest\n",
      "    manifest = Archive.restore_manifest(restore_path=source_file)\n",
      "  File \"<frozen eff.core.archive>\", line 657, in restore_manifest\n",
      "  File \"<frozen eff.core.archive>\", line 772, in extract\n",
      "TypeError: The indicated file '/servicemaker-dev/speechtotext_en_us_conformer_vdeployable_v4.0/Conformer-CTC-L-en-US-ASR-set-4p0.riva' is not an EFF archive\n"
     ]
    }
   ],
   "source": [
    "# Syntax: \n",
    "# riva-build <task-name> \\\n",
    "#     output-dir-for-rmir/model.rmir:key \\\n",
    "#     dir-for-riva/acoustic_model.riva:key \\\n",
    "#     --decoding_language_model_binary=lm_model.binary\n",
    "! docker run --rm --gpus 1 -v $ASR_MODEL_DIR:$ASR_MODEL_DIR_SM $RIVA_SM_CONTAINER -- \\\n",
    "    riva-build speech_recognition \\\n",
    "        $ASR_RMIR_SM:$KEY \\\n",
    "        $AM_SM:$KEY \\\n",
    "        --decoding_language_model_binary=$DECODING_LM_BINARY_SM \\\n",
    "        --decoding_vocab=$DECODING_VOCAB_SM \\\n",
    "        --wfst_tokenizer_model=$WFST_TOKENIZER_MODEL_SM \\\n",
    "        --wfst_verbalizer_model=$WFST_VERBALIZER_MODEL_SM \\\n",
    "        --name=conformer-ctc-en-US-asr-lm-itn-offline \\\n",
    "        --featurizer.use_utterance_norm_params=False \\\n",
    "        --featurizer.precalc_norm_time_steps=0 \\\n",
    "        --featurizer.precalc_norm_params=False \\\n",
    "        --ms_per_timestep=40 \\\n",
    "        --endpointing.start_history=200 \\\n",
    "        --endpointing.residue_blanks_at_start=-2 \\\n",
    "        --nn.fp16_needs_obey_precision_pass \\\n",
    "        --chunk_size=4.8 \\\n",
    "        --left_padding_size=1.6 \\\n",
    "        --right_padding_size=1.6 \\\n",
    "        --max_batch_size=16 \\\n",
    "        --featurizer.max_batch_size=512 \\\n",
    "        --featurizer.max_execution_batch_size=512 \\\n",
    "        --decoder_type=flashlight \\\n",
    "        --flashlight_decoder.asr_model_delay=-1 \\\n",
    "        --flashlight_decoder.lm_weight=0.8 \\\n",
    "        --flashlight_decoder.word_insertion_score=1.0 \\\n",
    "        --flashlight_decoder.beam_size=32 \\\n",
    "        --flashlight_decoder.beam_threshold=20. \\\n",
    "        --flashlight_decoder.num_tokenization=1 \\\n",
    "        --language_code=en-US \\\n",
    "        --offline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '/dli/task/asr-models/default-models/rmir/asr_lm_itn_offline.rmir': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# Check your work - the new language model RMIR file should be in the ASR_MODEL_DIR/rmir directory now\n",
    "!ls $ASR_MODEL_DIR/rmir/asr_lm_itn_offline.rmir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3.4 Exercise: Build a Punctuation and Capitalization RMIR\n",
    "\n",
    "Punctuation and capitalization RMIR models are built using `riva-build punctuation`.  The `docker run` portion is the same, and we use the same ServiceMaker container image. Here's the basic syntax for the `riva-build punctuation` command portion: \n",
    "\n",
    "```text\n",
    "riva-build punctuation \\\n",
    "    output-dir-for-rmir/punctuation_model.rmir:key \\\n",
    "    dir-for-riva/punctuation_model.riva:key \\\n",
    "    --language_code=<language, 2 letters>-<country, 2 letters>\n",
    "    --name=p_and_c_pipeline\n",
    "```\n",
    "\n",
    "which becomes\n",
    "\n",
    "```text\n",
    "riva-build punctuation \\\n",
    "    $PC_RMIR_SM:$KEY \\\n",
    "    $PC_SM:$KEY \\\n",
    "    --language_code=en-US \\\n",
    "    --name=p_and_c_pipeline\n",
    "```\n",
    "\n",
    "The syntax is similar, but instead of requiring language and acoustic models, we need the punctuation model as input and need to specify the language code.\n",
    "\n",
    "For this exercise, put the `docker run` and `riva-build` commands together to build the punctuation model.  If you get stuck, you can check the [solution](solutions/ex5.3.4.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Use docker run and riva-build to create a punctuation RMIR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# quick fix\n",
    "import os\n",
    "if not os.path.exists(\"asr-models/default-models/rmir/p_and_c.rmir\"):\n",
    "    ! docker run --rm --gpus 1 -v $ASR_MODEL_DIR:$ASR_MODEL_DIR_SM $RIVA_SM_CONTAINER -- \\\n",
    "        riva-build punctuation \\\n",
    "            $PC_RMIR_SM:$KEY \\\n",
    "            $PC_SM:$KEY \\\n",
    "            --language_code=en-US \\\n",
    "            --name=p_and_c_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '/dli/task/asr-models/default-models/rmir/p_and_c.rmir': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# Check your work - the new p&c RMIR file should be in the MODEL_LOC directory now\n",
    "!ls $ASR_MODEL_DIR/rmir/p_and_c.rmir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# 5.4 `riva-deploy`\n",
    "\n",
    "The deployment tool takes as input one or more RMIR files and a target model repository directory. It creates an ensemble configuration specifying the pipeline for the execution and writes all those assets to the output model repository directory.  For our project, we are using both the `asr_offline_binary_ngram_lm.rmir` and `p_and_c.rmir` files as input.  Our output directory is mapped to `$MODEL_LOC/models` on the host system. For more details, see the [Using riva-deploy](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/model-overview.html#using-riva-deploy-and-riva-speech-container-advanced) documentation.\n",
    "\n",
    "_Note: The files we need have been preloaded for the course to save time, because this step would otherwise take about 30 minutes.  The \"-f\" option has been removed to avoid overwriting the preloaded models._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================\n",
      "=== Riva Speech Skills ===\n",
      "==========================\n",
      "\n",
      "NVIDIA Release  (build 59018721)\n",
      "Copyright (c) 2016-2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
      "\n",
      "Copyright (c) 2018-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
      "\n",
      "https://developer.nvidia.com/tensorrt\n",
      "\n",
      "Various files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n",
      "\n",
      "This container image and its contents are governed by the NVIDIA Deep Learning Container License.\n",
      "By pulling and using the container, you accept the terms and conditions of this license:\n",
      "https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\n",
      "\n",
      "To install Python sample dependencies, run /opt/tensorrt/python/python_setup.sh\n",
      "\n",
      "To install the open-source samples corresponding to this TensorRT release version\n",
      "run /opt/tensorrt/install_opensource.sh.  To build the open source parsers,\n",
      "plugins, and samples for current top-of-tree on master or a different branch,\n",
      "run /opt/tensorrt/install_opensource.sh -b <branch>\n",
      "See https://github.com/NVIDIA/TensorRT for more information.\n",
      "\n",
      "NOTE: CUDA Forward Compatibility mode ENABLED.\n",
      "  Using CUDA 11.8 driver version 520.61.05 with kernel driver version 510.47.03.\n",
      "  See https://docs.nvidia.com/deploy/cuda-compatibility/ for details.\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/tarfile.py\", line 1674, in gzopen\n",
      "    t = cls.taropen(name, mode, fileobj, **kwargs)\n",
      "  File \"/usr/lib/python3.8/tarfile.py\", line 1651, in taropen\n",
      "    return cls(name, mode, fileobj, **kwargs)\n",
      "  File \"/usr/lib/python3.8/tarfile.py\", line 1514, in __init__\n",
      "    self.firstmember = self.next()\n",
      "  File \"/usr/lib/python3.8/tarfile.py\", line 2318, in next\n",
      "    tarinfo = self.tarinfo.fromtarfile(self)\n",
      "  File \"/usr/lib/python3.8/tarfile.py\", line 1104, in fromtarfile\n",
      "    buf = tarfile.fileobj.read(BLOCKSIZE)\n",
      "  File \"/usr/lib/python3.8/gzip.py\", line 292, in read\n",
      "    return self._buffer.read(size)\n",
      "  File \"/usr/lib/python3.8/_compression.py\", line 68, in readinto\n",
      "    data = self.read(len(byte_view))\n",
      "  File \"/usr/lib/python3.8/gzip.py\", line 479, in read\n",
      "    if not self._read_gzip_header():\n",
      "  File \"/usr/lib/python3.8/gzip.py\", line 427, in _read_gzip_header\n",
      "    raise BadGzipFile('Not a gzipped file (%r)' % magic)\n",
      "gzip.BadGzipFile: Not a gzipped file (b'<?')\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen eff.core.archive>\", line 764, in extract\n",
      "  File \"/usr/lib/python3.8/tarfile.py\", line 1621, in open\n",
      "    return func(name, filemode, fileobj, **kwargs)\n",
      "  File \"/usr/lib/python3.8/tarfile.py\", line 1678, in gzopen\n",
      "    raise ReadError(\"not a gzip file\")\n",
      "tarfile.ReadError: not a gzip file\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/riva-build\", line 8, in <module>\n",
      "    sys.exit(build())\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/servicemaker/cli/build.py\", line 96, in build\n",
      "    nm = model(input_filename, pipeline_config=pipeline_config, encryption_key=input_key)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/servicemaker/readers/tlt.py\", line 76, in __init__\n",
      "    read_eff_manifest(self, self.source_file)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/servicemaker/readers/tlt.py\", line 65, in read_eff_manifest\n",
      "    manifest = Archive.restore_manifest(restore_path=source_file)\n",
      "  File \"<frozen eff.core.archive>\", line 657, in restore_manifest\n",
      "  File \"<frozen eff.core.archive>\", line 772, in extract\n",
      "TypeError: The indicated file '/servicemaker-dev/punctuationcapitalization_en_us_bert_base_vdeployable_v3.0/bert-base_PnC_en-US_3.0.riva' is not an EFF archive\n"
     ]
    }
   ],
   "source": [
    "# Syntax: \n",
    "# riva-deploy -f \\\n",
    "#     dir-for-rmir/asr_model.rmir:key \\\n",
    "#     dir-for-rmir/p_and_cmodel.rmir:key \\\n",
    "#     output-dir-for-repository\n",
    "! docker run --rm --gpus 1 -v $ASR_MODEL_DIR:/data $RIVA_SM_CONTAINER -- \\\n",
    "    riva-deploy \\\n",
    "        /data/rmir/asr_lm_itn_offline.rmir:$KEY \\\n",
    "        /data/rmir/p_and_c.rmir:$KEY \\\n",
    "        /data/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your work - the new model files should be in the ASR_MODEL_DIR/models directory now\n",
    "!ls $ASR_MODEL_DIR/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5.5 Start the Riva Server\n",
    "After the model repository is generated, we are ready to start the Riva server. We've already downloaded the [Riva Quick Start](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/riva/resources/riva_quickstart) resource from NGC. <br>\n",
    "Set the path to the directory here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Riva Quick Start directory\n",
    "RIVA_DIR = \"/dli/task/riva_quickstart_v2.11.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `riva_quickstart` folder includes shell scripts to start the Riva server as well as the Riva Python API bindings for the client.  When running the `riva_init.sh` and `riva_start.sh` scripts the `config.sh` file is used as an argument to encapsulate the settings. To learn more about this workflow, check the [Deploy Process documentation](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/model-overview.html#deploy-process).\n",
    "\n",
    "For our project, we will not run `riva_init.sh` because we've already used `riva-deploy`. We can move on directly to the `riva_start.sh` script.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5.5.1 Exercise: Configure the `config.sh` File\n",
    "Open the [config.sh](riva_quickstart_v2.11.0/config.sh) file to edit it.  You'll need to make two general modifications.\n",
    "\n",
    "- Modify it so that we are only enabling the ASR service (not NLP or TTS).  \n",
    "- Modify the path to the model repository (`riva_model_loc`) generated in the previous step among other configurations.  For example, if the model repository is generated at `$ASR_MODEL_DIR/models`, then you can specify `riva_model_loc` as the same directory as `ASR_MODEL_DIR`.  Use the literal value, which for this environment is `\"/dli/task/asr-models/default-models\"`.\n",
    "\n",
    "Use the following snippet as a guide, then check your work before you attempt to start the server.  You can also take a look at the [solution](solutions/ex5.5.1_config.sh) if you get stuck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### config.sh snippet\n",
    "```bash\n",
    "# Enable or Disable Riva Services\n",
    "service_enabled_asr=true \n",
    "service_enabled_nlp=true # MAKE CHANGES HERE - SET TO FALSE\n",
    "service_enabled_tts=true # MAKE CHANGES HERE - SET TO FALSE\n",
    "service_enabled_nmt=true # MAKE CHANGES HERE - SET TO FALSE\n",
    "\n",
    "...\n",
    "\n",
    "# Locations to use for storing models artifacts\n",
    "#\n",
    "# If an absolute path is specified, the data will be written to that location\n",
    "# Otherwise, a docker volume will be used (default).\n",
    "#\n",
    "# riva_init.sh will create a `rmir` and `models` directory in the volume or\n",
    "# path specified.\n",
    "#\n",
    "# RMIR ($riva_model_loc/rmir)\n",
    "# Riva uses an intermediate representation (RMIR) for models\n",
    "# that are ready to deploy but not yet fully optimized for deployment. Pretrained\n",
    "# versions can be obtained from NGC (by specifying NGC models below) and will be\n",
    "# downloaded to $riva_model_loc/rmir by `riva_init.sh`\n",
    "#\n",
    "# Custom models produced by NeMo or TLT and prepared using riva-build\n",
    "# may also be copied manually to this location $(riva_model_loc/rmir).\n",
    "#\n",
    "# Models ($riva_model_loc/models)\n",
    "# During the riva_init process, the RMIR files in $riva_model_loc/rmir\n",
    "# are inspected and optimized for deployment. The optimized versions are\n",
    "# stored in $riva_model_loc/models. The riva server exclusively uses these\n",
    "# optimized versions.\n",
    "riva_model_loc=\"riva-model-repo\"  ## MAKE CHANGES HERE - REPLACE WITH \n",
    "\n",
    "if [[ $riva_target_gpu_family == \"tegra\" ]]; then\n",
    "    riva_model_loc=\"`pwd`/model_repository\"\n",
    "fi\n",
    "\n",
    "# The default RMIRs are downloaded from NGC by default in the above $riva_rmir_loc directory\n",
    "# If you'd like to skip the download from NGC and use the existing RMIRs in the $riva_rmir_loc\n",
    "# then set the below $use_existing_rmirs flag to true. You can also deploy your set of custom\n",
    "# RMIRs by keeping them in the riva_rmir_loc dir and use this quickstart script with the\n",
    "# below flag to deploy them all together.\n",
    "use_existing_rmirs=false          ## MAKE CHANGES HERE - SET TO TRUE                  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# quick fix!\n",
    "! cp solutions/ex5.5.1_config.sh $RIVA_DIR/config.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check your work.  Compare with the solution.  Exact matches provide no output.\n",
    "! diff solutions/ex5.5.1_config.sh $RIVA_DIR/config.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5.2 Start the Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure you have permission to execute these scripts\n",
    "! cd $RIVA_DIR && chmod +x *.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start the server.  This should take about 30 seconds.\n",
    "! cd $RIVA_DIR && ./riva_start.sh config.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5.6 Run Inference\n",
    "After the Riva server is up and running with our models, we can send inference requests querying the server using the Riva Python API bindings we used in the Quick Start example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell queries the Riva server (using gRPC) to yield a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import riva.client\n",
    "\n",
    "def run_inference(audio_file, server = \"localhost:50051\"):\n",
    "    wf = wave.open(audio_file, 'rb')\n",
    "    with open(audio_file, 'rb') as fh:\n",
    "        data = fh.read()\n",
    "\n",
    "    channel = grpc.insecure_channel(server)\n",
    "    client = rasr_srv.RivaSpeechRecognitionStub(channel)\n",
    "    config = rasr.RecognitionConfig(\n",
    "        encoding=ra.AudioEncoding.LINEAR_PCM,\n",
    "        sample_rate_hertz=wf.getframerate(),\n",
    "        language_code=\"en-US\",\n",
    "        max_alternatives=1,\n",
    "        enable_automatic_punctuation=True,\n",
    "        audio_channel_count=1\n",
    "    )\n",
    "\n",
    "    request = rasr.RecognizeRequest(config=config, audio=data)\n",
    "\n",
    "    response = client.Recognize(request)\n",
    "    print(response.results[0].alternatives[0].transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the Riva Server and Run Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, define a helper function for obtaining an audio file's encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoding(audio_file):\n",
    "    file_extension = audio_file.split('.')[-1]\n",
    "    if file_extension == 'wav':\n",
    "        encoding = riva.client.AudioEncoding.LINEAR_PCM\n",
    "    elif file_extension == 'flac':\n",
    "        encoding = riva.client.AudioEncoding.FLAC\n",
    "    elif file_extension == 'alaw':\n",
    "        encoding = riva.client.AudioEncoding.ALAW\n",
    "    elif file_extension == 'mulaw':\n",
    "        encoding = riva.client.AudioEncoding.MULAW\n",
    "    else:\n",
    "        raise Exception(f'Audio format \".{file_extension}\" not supported.')\n",
    "    return encoding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling this inference function queries the Riva server (using gRPC) to transcribe an audio file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(audio_file, server='localhost:50051', print_full_response=False):\n",
    "    with open(audio_file, 'rb') as fh:\n",
    "        data = fh.read()\n",
    "\n",
    "    auth = riva.client.Auth(uri=server)\n",
    "    client = riva.client.ASRService(auth)\n",
    "    config = riva.client.RecognitionConfig(\n",
    "        encoding=get_encoding(audio_file),\n",
    "        language_code=\"en-US\",\n",
    "        max_alternatives=1,\n",
    "        enable_automatic_punctuation=True,\n",
    "    )\n",
    "    riva.client.add_audio_file_specs_to_config(config, audio_file)\n",
    "\n",
    "    response = client.offline_recognize(data, config)\n",
    "    if print_full_response: \n",
    "        print(response)\n",
    "    else:\n",
    "        print(\"ASR transcript:\")\n",
    "        print(response.results[0].alternatives[0].transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's play the audio file on which we will run inference on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import IPython.display as ipd\n",
    "\n",
    "audio_file = \"audio_samples/test.wav\"\n",
    "# Load a sample audio file from local disk\n",
    "# This example uses a .wav file with LINEAR_PCM encoding.\n",
    "with io.open(audio_file, 'rb') as fh:\n",
    "    content = fh.read()\n",
    "ipd.Audio(audio_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run inference and compare the transcript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_inference(audio_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see and hear, our ASR pipeline, constructed from pretrained, OOTB components, transcribed the sample audio file perfectly. However, the acoustic model was trained on US English. In the later notebook, we'll explore how to fine-tune the acoustic model to better transcribe audio from Nigerian English speakers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5.7 Stop the Riva Server\n",
    "Before moving on, shut down the Riva server to stop the containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! cd $RIVA_DIR && ./riva_stop.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<h2 style=\"color:green;\">Congratulations!</h2>\n",
    "\n",
    "You've learned how to:\n",
    "- Download models from NGC\n",
    "- Build a Riva pipeline using Riva ServiceMaker to take a pretrained `.riva` file (exported from NeMo) and convert it to `.rmir` file\n",
    "- Deploy the model locally on the Riva server\n",
    "- Send inference requests from a demo client using Riva API bindings\n",
    "\n",
    "Related tutorials:\n",
    "- [How do I use Riva ASR APIs with out-of-the-box models?](https://github.com/nvidia-riva/tutorials/blob/main/asr-basics.ipynb)<br>\n",
    "- [How To Train, Evaluate, and Fine-Tune an n-gram Language Model with NVIDIA NeMo](https://github.com/nvidia-riva/tutorials/blob/main/asr-python-advanced-nemo-ngram-training-and-finetuning.ipynb)<br>\n",
    "\n",
    "Next, let's explore how to improve inference at runtime in [the Word Boosting notebook](006_Word_Boosting.ipynb). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
